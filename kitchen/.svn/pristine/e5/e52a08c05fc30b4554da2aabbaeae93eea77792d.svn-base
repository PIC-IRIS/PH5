#!/usr/bin/env pnpython3
# -*- coding: iso-8859-15 -*-
#
#   Read Fairfield SEG-D (Version 1.6) from the Sweetwater experiment.
#   Write PH5
#
#   Steve Azevedo, May 2014
#

PROG_VERSION = "2016.011 Developmental"

MAX_PH5_BYTES = 1073741824 * 100.   #   100 GB (1024 X 1024 X 1024 X 2)

import os, sys, logging, time, json, re
from math import modf
import Experiment, columns, SegdReader
from pyproj import Proj, transform

os.environ['TZ'] = 'GMT'
time.tzset ()

APPEND = 26   #   Number of SEG-D events to append to make 1 ph5 event.

DAS_INFO = {}
MAP_INFO = {}
#   Current raw file processing
F = None

miniPH5RE = re.compile (".*miniPH5_(\d\d\d\d\d)\.ph5")

#LSB = 6.402437066e-6   #   From Malcolm UCSD
#LSB = 2500. / (2**23)   #   0dB
#LSB = 625. / (2**23)    #   12dB
#LSB = 156. / (2**23)    #   24dB
LSB = 39. / (2**23)     #   36dB = 39mV full scale

#
#   To hold table rows and keys
#
class rows_keys (object) :
    __slots__ = ('rows', 'keys')
    def __init__ (self, rows = None, keys = None) :
        self.rows = rows
        self.keys = keys
        
    def set (self, rows = None, keys = None) :
        if rows != None : self.rows = rows
        if keys != None : self.keys = keys

class index_t_info (object) :
    __slots__ = ('das', 'ph5file', 'ph5path', 'startepoch', 'stopepoch')
    def __init__ (self, das, ph5file, ph5path, startepoch, stopepoch) :
        self.das        = das
        self.ph5file    = ph5file
        self.ph5path    = ph5path
        self.startepoch = startepoch
        self.stopepoch  = stopepoch

class resp (object) :
    __slots__ = ('lines', 'keys', 't')
    def __init__ (self, t) :
        self.t = t
        self.update ()
        
    def update (self) :
        self.lines, self.keys = self.t.read_responses ()
        
    def match (self, bw, gain) :
        #print self.lines
        for l in self.lines :
            if l['bit_weight/value_d'] == bw and l['gain/value_i'] == gain :
                return l['n_i']
            
        return -1
    
    def next_i (self) :
        return len (self.lines)

def read_infile (infile) :
    '''   Read list of input SEG-D files from a file   '''
    global FILES
    
    def fn_sort (a, b) :
        #print os.path.basename (a), os.path.basename (b)
        return cmp (os.path.basename (a), os.path.basename (b))
    
    try :
        fh = file (infile)
    except :
        sys.stderr.write ("Warning: Failed to open %s\n" % infile)
        return
        
    while True :
        line = fh.readline ()
        if not line : break
        line = line.strip ()
        if not line : continue
        if line[0] == '#' : continue
        FILES.append (line) 
        
    FILES.sort (fn_sort)
        
def get_args () :
    global PH5, FILES, EVERY, NUM_MINI, TSPF, UTM, FIRST_MINI
    
    TSPF = False
    
    from optparse import OptionParser
    oparser = OptionParser ()
    
    oparser.usage = "Version: {0} Usage: segd2ph5 [options]".format (PROG_VERSION)
    
    oparser.add_option ("-r", "--raw", dest = "rawfile",
                        help="Fairfield SEG-D v1.6 file.", metavar="raw_file")    
    
    oparser.add_option ("-f", action="store", dest="infile", type="string",
                        help = "File containing list of Fairfield SEG-D v1.6 file names.")
    
    oparser.add_option ("-n", "--nickname", dest = "outfile",
                        help="The ph5 file prefix (experiment nick name).",
                        metavar = "output_file_prefix")
    
    oparser.add_option ("-U", "--UTM", dest = "utm_zone",
                        help="Locations in SEG-D file are UTM, --UTM=utmzone.",
                        type = 'int', default = 0,
                        metavar = "utm_zone")    
    
    oparser.add_option ("-T", "--TSPF", dest = "texas_spc",
                        help="Locations are in texas state plane coordinates.",
                        action='store_true', default=False)
    
    oparser.add_option ("-M", "--num_mini", dest = "num_mini",
                        help = "Create a given number of miniPH5_xxxxx.ph5 files.",
                        metavar = "num_mini", type = 'int', default = None)
    
    oparser.add_option ("-S", "--first_mini", dest = "first_mini",
                        help = "The index of the first miniPH5_xxxxx.ph5 file.",
                        metavar = "first_mini", type = 'int', default = 1)
    
    oparser.add_option ("-E", "--allevents", action="store_true", dest="all_events",
                            default=False, metavar="all_events")    
    
    options, args = oparser.parse_args ()

    FILES = []
    PH5 = None
    
    EVERY = options.all_events
    NUM_MINI = options.num_mini
    FIRST_MINI = options.first_mini
    UTM = options.utm_zone
    TSPF = options.texas_spc
    
    if options.infile != None :
        read_infile (options.infile)
        
    elif options.rawfile != None :
        FILES.append (options.rawfile)    
    
    if len (FILES) == 0 :
        sys.stderr.write ("Error: No input file given.\n")
        sys.exit ()
        
    #   Set output file
    if options.outfile != None :
        PH5 = options.outfile
    else :
        sys.stderr.write ("Error: No outfile (PH5) given.\n")
        sys.exit ()   
        
    logging.basicConfig (
        filename = os.path.join ('.', "segd2ph5.log"),
        format = "%(asctime)s %(message)s",
        level = logging.INFO
    )
    #   Need to process in order: R309_674.1.0.rg16, 309 == line, 674 = receiver point, 1 = first file
    #   Sorted where the file list is read...
    #FILES.sort ()

def initializeExperiment () :
    global EX
    
    EX = Experiment.ExperimentGroup (nickname = PH5)
    EDIT = True
    EX.ph5open (EDIT)
    EX.initgroup ()
    
def openPH5 (filename) :
    '''   Open PH5 file, miniPH5_xxxxx.ph5   '''
    try :
        if EXREC.ph5.isopen :
            if EXREC.filename != filename :
                EXREC.ph5close ()
            else :
                return EXREC
    except :
        pass    
    #sys.stderr.write ("***   Opening: {0} ".format (filename))
    exrec = Experiment.ExperimentGroup (nickname = filename)
    exrec.ph5open (True)
    exrec.initgroup ()
    return exrec

def update_index_t_info (starttime, samples, sps) :
    '''   Update info that gets saved in Index_t   '''
    global DAS_INFO, MAP_INFO
    
    ph5file = EXREC.filename
    ph5path = '/Experiment_g/Receivers_g/' + EXREC.ph5_g_receivers.current_g_das._v_name
    ph5map = '/Experiment_g/Maps_g/' + EXREC.ph5_g_maps.current_g_das._v_name
    das = ph5path[32:]
    stoptime = starttime + (float (samples) / float (sps))
    di = index_t_info (das, ph5file, ph5path, starttime, stoptime)
    dm = index_t_info (das, ph5file, ph5map, starttime, stoptime)
    if not DAS_INFO.has_key (das) :
        DAS_INFO[das] = []
        MAP_INFO[das] = []
        
    DAS_INFO[das].append (di)
    MAP_INFO[das].append (dm)
    logging.info ("DAS: {0} File: {1} First Sample: {2} Last Sample: {3}".format (das, ph5file, time.ctime (starttime), time.ctime (stoptime)))

def update_external_references () :
    '''   Update external references in master.ph5 to miniPH5 files in Receivers_t    '''
    global F
    #sys.stderr.write ("Updating external references...\n"); sys.stderr.flush ()
    logging.info ("Updating external references...")
    n = 0
    for i in INDEX_T_DAS.rows :
        external_file = i['external_file_name_s'][2:]
        external_path = i['hdf5_path_s']
        das = i['serial_number_s']
        target = external_file + ':' + external_path
        external_group = external_path.split ('/')[3]
        ###print external_file, external_path, das, target, external_group
        
        #   Nuke old node
        try :
            group_node = EX.ph5.get_node (external_path)
            group_node.remove ()
        except Exception, e :
            pass
            #print "DAS nuke ", e
            
        #   Re-create node
        try :
            EX.ph5.create_external_link ('/Experiment_g/Receivers_g', external_group, target)
            n += 1
        except Exception, e :
            #pass
            sys.stderr.write ("{0}\n".format (e))
            
        #sys.exit ()
    #sys.stderr.write ("done, {0} das nodes recreated.\n".format (n))
    sys.stdout.write (":<Finished>: {0}\n".format (F)); sys.stdout.flush ()
    logging.info ("done, {0} das nodes recreated.\n".format (n))
    
    n = 0
    for i in INDEX_T_MAP.rows :
        #   XXX
        #keys = i.keys ()
        #keys.sort ()
        #for k in keys :
            #print k, i[k]
            
        external_file = i['external_file_name_s'][2:]
        external_path = i['hdf5_path_s']
        das = i['serial_number_s']
        target = external_file + ':' + external_path
        external_group = external_path.split ('/')[3]
        ###print external_file, external_path, das, target, external_group
        
        #   Nuke old node
        try :
            group_node = EX.ph5.get_node (external_path)
            group_node.remove ()
        except Exception, e :
            pass
            #print "MAP nuke ", e
            
        #   Re-create node
        try :
            EX.ph5.create_external_link ('/Experiment_g/Maps_g', external_group, target)
            n += 1
        except Exception, e :
            #pass
            sys.stderr.write ("{0}\n".format (e))
            
        #sys.exit ()
    #sys.stderr.write ("done, {0} map nodes recreated.\n".format (n))
    logging.info ("done, {0} map nodes recreated.\n".format (n))   
    
##@profile
#def get_current_data_only (size_of_data, das = None) :
    #'''   Return opened file handle for data only PH5 file that will be
          #less than MAX_PH5_BYTES after raw data is added to it.
    #'''
    
    #newest = 0
    #newestfile = ''
    ##   Get the most recent data only PH5 file or match DAS serialnumber
    #for index_t in INDEX_T_DAS.rows :
        ##   This DAS already exists in a ph5 file
        #if index_t['serial_number_s'] == str (das) :
            #newestfile = index_t['external_file_name_s']
            #newestfile = newestfile.replace ('.ph5', '')
            #newestfile = newestfile.replace ('./', '')
            #return openPH5 (newestfile)
        ##   Find most recent ph5 file
        #if index_t['time_stamp/epoch_l'] > newest :
            #newest = index_t['time_stamp/epoch_l']
            #newestfile = index_t['external_file_name_s']
            #newestfile = newestfile.replace ('.ph5', '')
            #newestfile = newestfile.replace ('./', '')
            
    ##print newest, newestfile
    #if not newestfile :
        ##   This is the first file added
        #return openPH5 ('miniPH5_00001')
    
    #size_of_exrec = os.path.getsize (newestfile + '.ph5')
    ##print size_of_data, size_of_exrec, size_of_data + size_of_exrec, MAX_PH5_BYTES
    #if (size_of_data + size_of_exrec) > MAX_PH5_BYTES :
        #newestfile = "miniPH5_{0:05d}".format (int (newestfile[8:13]) + 1)

    
    #return openPH5 (newestfile)


def get_current_data_only (size_of_data, das = None) :
    '''   Return opened file handle for data only PH5 file that will be
          less than MAX_PH5_BYTES after raw data is added to it.
    '''
    #global NM
    #global INDEX_T, CURRENT_DAS
    def sstripp (s) :
        s = s.replace ('.ph5', '')
        s = s.replace ('./', '')
        return s
    
    def smallest () :
        '''   Return the name of the smallest miniPH5_xxxxx.ph5   '''
        minifiles = filter (miniPH5RE.match, os.listdir ('.'))
        
        tiny = minifiles[0]
        for f in minifiles :
            if os.path.getsize (f) < os.path.getsize (tiny) :
                tiny = f
                
        return tiny
            
    das = str (das)
    newest = 0
    newestfile = ''
    #   Get the most recent data only PH5 file or match DAS serialnumber
    n = 0
    for index_t in INDEX_T_DAS.rows :
        #   This DAS already exists in a ph5 file
        if index_t['serial_number_s'] == das :
            newestfile = sstripp (index_t['external_file_name_s'])
            return openPH5 (newestfile) 
        #   miniPH5_xxxxx.ph5 with largest xxxxx
        mh = miniPH5RE.match (index_t['external_file_name_s'])
        if n < int (mh.groups ()[0]) :
            newestfile = sstripp (index_t['external_file_name_s'])
            n = int (mh.groups ()[0])
            
    if not newestfile :
        #   This is the first file added
        return openPH5 ('miniPH5_{0:05d}'.format (FIRST_MINI))
    
    size_of_exrec = os.path.getsize (newestfile + '.ph5')
    #print size_of_data, size_of_exrec, size_of_data + size_of_exrec, MAX_PH5_BYTES
    if NUM_MINI != None :
        fm = FIRST_MINI - 1
        if (int (newestfile[8:13]) - fm) < NUM_MINI :
            newestfile = "miniPH5_{0:05d}".format (int (newestfile[8:13]) + 1)
        else :
            small = sstripp (smallest ())
            return openPH5 (small)
        
    elif (size_of_data + size_of_exrec) > MAX_PH5_BYTES :
        newestfile = "miniPH5_{0:05d}".format (int (newestfile[8:13]) + 1)

    return openPH5 (newestfile)

def getLOG () :
    '''   Create a open a new and unique header file under Maps_g/Das_g_
                                                                 /Sta_g_
                                                                 /Evt_g_
                                                                         /Hdr_a_
    '''
    current_das = EXREC.ph5_g_receivers.get_das_name ()
    g = EXREC.ph5_g_maps.newdas ('Das_g_', current_das)
    EXREC.ph5_g_maps.setcurrent (g)
    try :
        name = EXREC.ph5_g_maps.nextarray ('Hdr_a_')
    except TypeError :
        return None
    
    log_array = EXREC.ph5_g_maps.newearray (name, description = "SEG-D header entries: {0}".format (Das))
    
    return log_array, name

def process_traces (rh, th, tr) :
    '''
    '''
    global DN
    def process_das () :
        '''
        '''
        p_das_t = {}
        '''  Das_t
                receiver_table_n_i
                response_table_n_i
                time_table_n_i
                time/
                    type_s      
                    epoch_l
                    ascii_s
                    micro_seconds_i
                event_number_i
                channel_number_i
                sample_rate_i
                sample_rate_multiplier_i
                sample_count_i
                stream_number_i
                raw_file_name_s
                array_name_data_a
                array_name_SOH_a
                array_name_event_a
                array_name_log_a 
        '''
        #   Check to see if group exists for this das, if not build it
        das_g, das_t, receiver_t, time_t = EXREC.ph5_g_receivers.newdas (str (Das))
        #   Build maps group (XXX)
        map_g = EXREC.ph5_g_maps.newdas ('Das_g_', str (Das))     
        p_das_t['receiver_table_n_i'] = 0   #   0 -> Z, 1 -> N (along line), 2 -> E (cross line)
        p_das_t['response_table_n_i'] = None
        p_das_t['time_table_n_i'] = 0
        p_das_t['time/type_s'] = 'BOTH'
        #trace_epoch = th.trace_header_N[2].gps_tim1 * 4294967296 + th.trace_header_N[2].gps_tim2
        trace_epoch = th.trace_header_N[2].shot_epoch
        f, i = modf (trace_epoch / 1000000.)
        p_das_t['time/epoch_l'] = int (i)
        p_das_t['time/ascii_s'] = time.ctime (p_das_t['time/epoch_l'])
        p_das_t['time/micro_seconds_i'] = int (f * 1000000.)
        p_das_t['event_number_i'] = th.trace_header_N[1].shot_point
        p_das_t['channel_number_i'] = 1
        p_das_t['sample_rate_i'] = SD.sample_rate
        p_das_t['sample_rate_multiplier_i'] = 1
        p_das_t['sample_count_i'] = len (tr)
        p_das_t['stream_number_i'] = 1
        p_das_t['raw_file_name_s'] = os.path.basename (SD.name ())
        p_das_t['array_name_data_a'] = EXREC.ph5_g_receivers.nextarray ('Data_a_')
        #p_das_t['array_name_SOH_a'] = None
        #p_das_t['array_name_event_a'] = None
        #p_das_t['array_name_log_a'] = None
        p_response_t = {}
        '''
            n_i
            gain/
                units_s
                value_i
            bit_weight/
                units_s       
                value_d        
            response_file_a
        '''
        n_i = RESP.match (LSB, th.trace_header_N[3].preamp_gain_db)
        p_response_t['gain/units_s'] = 'dB'
        p_response_t['gain/value_i'] = th.trace_header_N[3].preamp_gain_db
        p_response_t['bit_weight/units_s'] = 'mV/count'
        p_response_t['bit_weight/value_d'] = LSB
        if n_i < 0 :
            n_i = RESP.next_i ()
            p_response_t['n_i'] = n_i
            EX.ph5_g_responses.populateResponse_t (p_response_t)
            RESP.update ()
        p_das_t['response_table_n_i'] = n_i
        EXREC.ph5_g_receivers.populateDas_t (p_das_t)
        des = "Epoch: " + str (p_das_t['time/epoch_l']) + " Channel: " + str (p_das_t['channel_number_i'])
        #   Write trace data here
        try :
            #   Convert to counts
            tr_counts = tr / LSB
            EXREC.ph5_g_receivers.newarray (p_das_t['array_name_data_a'], tr_counts, dtype = 'int32', description = des)
        except Exception as e :
            #   Failed, leave as float
            p_response_t['bit_weight/value_d'] = 1.
            EXREC.ph5_g_receivers.newarray (p_das_t['array_name_data_a'], tr, dtype = 'float32', description = des)
            sys.stderr.write ("Warning: Could not convert trace to counts.\n{0}".format (e))
        #
        update_index_t_info (p_das_t['time/epoch_l'] + (float (p_das_t['time/micro_seconds_i']) / 1000000.), p_das_t['sample_count_i'], p_das_t['sample_rate_i'] / p_das_t['sample_rate_multiplier_i'])
        
    def process_array () :
        p_array_t = {}
        
        def seen_sta () :
            if not ARRAY_T[line] :
                return False
            sta = ARRAY_T[line][-1]['id_s']
            if sta == p_array_t['id_s'] :
                return True
            else :
                return False
            
        '''
            deploy_time/
                type_s
                epoch_l
                ascii_s
                micro_seconds_i
            pickup_time/
                type_s
                epoch_l
                ascii_s
                micro_seconds_i
            id_s
            das/
                manufacturer_s
                model_s
                serial_number_s
                notes_s
            sensor/
                manufacturer_s
                model_s
                serial_number_s
                notes_s
            location/
                coordinate_system_s
                projection_s
                ellipsoid_s
                X/
                    units_s
                    value_d
                Y/
                    units_s
                    value_d
                Z/
                    units_s
                    value_d
                description_s
            channel_number_i
            description_s
        '''
        p_array_t['deploy_time/type_s'] = 'BOTH'
        f, i = modf (SD.reel_headers.extended_header_1.epoch_deploy / 1000000.)
        p_array_t['deploy_time/epoch_l'] = int (i)
        p_array_t['deploy_time/ascii_s'] = time.ctime (int(i))
        p_array_t['deploy_time/micro_seconds_i'] = int (f * 1000000.)
        p_array_t['pickup_time/type_s'] = 'BOTH'
        f, i = modf (SD.reel_headers.extended_header_1.epoch_pickup / 1000000.)
        p_array_t['pickup_time/epoch_l'] = int (i)
        p_array_t['pickup_time/ascii_s'] = time.ctime (int(i))
        p_array_t['pickup_time/micro_seconds_i'] = int (f * 1000000.)
        p_array_t['id_s'] = Das
        p_array_t['das/manufacturer_s'] = 'FairfieldNodal'
        p_array_t['das/model_s'] = 'zland-1C'
        p_array_t['das/serial_number_s'] = Das
        p_array_t['das/notes_s'] = "manufacturer and model not read from data file."
        p_array_t['sensor/manufacturer_s'] = 'Geo Space'
        p_array_t['sensor/model_s'] = 'GS-30CT'
        p_array_t['sensor/notes_s'] = "manufacturer and model not read from data file."
        p_array_t['location/description_s'] = "Converted from Texas State Plane FIPS zone 4202"
        p_array_t['location/coordinate_system_s'] = 'geographic'
        p_array_t['location/projection_s'] = 'WGS84'
        p_array_t['location/X/units_s'] = 'degrees'
        p_array_t['location/X/value_d'] = LON
        p_array_t['location/Y/units_s'] = 'degrees'
        p_array_t['location/Y/value_d'] = LAT
        p_array_t['location/Z/units_s'] = 'feet'
        p_array_t['location/Z/value_d'] = SD.trace_headers.trace_header_N[4].receiver_point_depth_final / 10.
        p_array_t['channel_number_i'] = 1
        p_array_t['description_s'] = str (SD.trace_headers.trace_header_N[4].line_number)
        
        line = SD.trace_headers.trace_header_N[4].line_number
        if not ARRAY_T.has_key (line) :
            ARRAY_T[line] = []
        
        if not seen_sta () :  
            ARRAY_T[line].append (p_array_t)
            DN = True
        
    def process_reel_headers () :
        '''   Save receiver record header information in Maps_g/Das_g_xxxxxxx/Hdr_a_xxxx file   '''
        def process (hdr, header_type) :
            l = [{'FileType':'SEG-D', 'HeaderType':header_type},hdr]
            log_array.append (json.dumps (l, sort_keys=True, indent=4).split ('\n'))
            
        log_array, log_name = getLOG ()
        #   General header 1
        process (rh.general_header_block_1, 'General 1')
        #   General header 1
        process (rh.general_header_block_2, 'General 2')
        #   General header N
        for i in range (len (rh.general_header_block_N)) :
            ht = "General {0}".format (i + 3)
            process (rh.general_header_block_N[i], ht)
        #   Channel set descriptors
        for i in range (len (rh.channel_set_descriptor)) :
            ht = "Channel Set {0}".format (i + 1)
            process (rh.channel_set_descriptor, ht)
        #   Extended header 1
        process (rh.extended_header_1, "Extended 1")
        #   Extended header 2
        process (rh.extended_header_2, "Extended 2")
        #   Extended header 3
        process (rh.extended_header_3, "Extended 3")
        #   Extended header 4 - n
        for i in range (len (rh.extended_header_4)) :
            ht = "Extended 4 ({0})".format (i + 1)
            process (rh.extended_header_4[i], ht)
        #   External header
        process (rh.external_header, "External Header")
        #   External header shot
        for i in range (len (rh.external_header_shot)) :
            ht = "External Shot {0}".format (i + 1)
            process (rh.external_header_shot[i], ht)
        
    def process_trace_header () :
        '''   Save trace header information in Maps_g/Das_g_xxxxxxx/Hdr_a_xxxx file   '''
        def process (hdr, header_type) :
            l = [{'FileType':'SEG-D', 'HeaderType':'trace', 'HeaderSubType':header_type},hdr]
            log_array.append (json.dumps (l, sort_keys=True, indent=4).split ('\n'))
            
        log_array, log_name = getLOG ()
        
        process (th.trace_header, "Trace Header")
        for i in range (len (th.trace_header_N)) :
            ht = "Header N-{0}".format (i + 1)
            process (th.trace_header_N[i], ht)
        
    #
    #
    #
    process_das ()
    if not DN : 
        process_array ()
        process_reel_headers ()
        
    process_trace_header ()

def write_arrays (Array_t) :
    '''   Write /Experiment_g/Sorts_g/Array_t_xxx   '''
    def station_cmp (x, y) :
        return cmp (x['id_s'], y['id_s'])
    
    keys = Array_t.keys ()
    keys.sort ()
    for k in keys :
        name = EX.ph5_g_sorts.nextName ()
        a = EX.ph5_g_sorts.newSort (name)
        Array = Array_t[k]
        Array.sort (station_cmp)
        for array_t in Array :
            columns.populate (a, array_t)
            
def writeINDEX () :
    '''   Write /Experiment_g/Receivers_g/Index_t   '''
    global DAS_INFO, MAP_INFO, INDEX_T_DAS, INDEX_T_MAP
    
    dass = DAS_INFO.keys ()
    dass.sort ()
    
    for das in dass :
        di = {}
        mi = {}
        start = sys.maxint
        stop = 0.        
        dm = [(d, m) for d in DAS_INFO[das] for m in MAP_INFO[das]]
        for d, m in dm :
            di['external_file_name_s'] = d.ph5file
            mi['external_file_name_s'] = m.ph5file
            di['hdf5_path_s'] = d.ph5path
            mi['hdf5_path_s'] = m.ph5path
            di['serial_number_s'] = das
            mi['serial_number_s'] = das
            if d.startepoch < start :
                start = d.startepoch
                
            if d.stopepoch > stop :
                stop = d.stopepoch
                
        di['time_stamp/epoch_l'] = int (time.time ())
        mi['time_stamp/epoch_l'] = int (time.time ())
        di['time_stamp/micro_seconds_i'] = 0
        mi['time_stamp/micro_seconds_i'] = 0
        di['time_stamp/type_s'] = 'BOTH'
        mi['time_stamp/type_s'] = 'BOTH'
        di['time_stamp/ascii_s'] = time.ctime (di['time_stamp/epoch_l'])
        mi['time_stamp/ascii_s'] = time.ctime (mi['time_stamp/epoch_l'])
        
        di['start_time/epoch_l'] = int (modf (start)[1])
        mi['start_time/epoch_l'] = int (modf (start)[1])
        di['start_time/micro_seconds_i'] = int (modf (start)[0] * 1000000)
        mi['start_time/micro_seconds_i'] = int (modf (start)[0] * 1000000)
        di['start_time/type_s'] = 'BOTH'
        mi['start_time/type_s'] = 'BOTH'
        di['start_time/ascii_s'] = time.ctime (start)
        mi['start_time/ascii_s'] = time.ctime (start)
        
        di['end_time/epoch_l'] = modf (stop)[1]
        mi['end_time/epoch_l'] = modf (stop)[1]
        di['end_time/micro_seconds_i'] = int (modf (stop)[0] * 1000000)
        mi['end_time/micro_seconds_i'] = int (modf (stop)[0] * 1000000)
        di['end_time/type_s'] = 'BOTH'
        mi['end_time/type_s'] = 'BOTH'
        di['end_time/ascii_s'] = time.ctime (stop)
        mi['end_time/ascii_s'] = time.ctime (stop)
                
        EX.ph5_g_receivers.populateIndex_t (di)
        EX.ph5_g_maps.populateIndex_t (mi)
            
    rows, keys = EX.ph5_g_receivers.read_index ()
    INDEX_T_DAS = rows_keys (rows, keys)
    
    rows, keys = EX.ph5_g_maps.read_index ()
    INDEX_T_MAP = rows_keys (rows, keys)    
    
    DAS_INFO = {}
    MAP_INFO = {}
    
def txncsptolatlon (northing, easting) :
    '''
       Sweetwater
       Convert texas state plane coordinates in feet to 
       geographic coordinates, WGS84.
    '''
    #   Texas NC state plane feet Zone 4202
    sp = Proj (init='epsg:32038')
    #   WGS84, geographic
    wgs = Proj (init='epsg:4326', proj='latlong')
    #   Texas SP coordinates: survey foot is 1200/3937 meters
    lon, lat = transform (sp, wgs, easting * 0.30480060960121924, northing * 0.30480060960121924)
    
    return lat, lon

def utmcsptolatlon (northing, easting) :
    '''
       Mount Saint Helens
       Convert UTM to
       geographic coordinates, WGS84.
    '''
    #   UTM
    utmc = Proj (proj='utm', zone=UTM, ellps='WGS84')
    #   WGS84, geographic
    wgs = Proj (init='epsg:4326', proj='latlong')
    #
    lon, lat = transform (utmc, wgs, easting, northing)

    return lat, lon
    
if __name__ == '__main__' :
    import time
    then = time.time ()
    from numpy import append as npappend
    
    def prof () :
        global RESP, INDEX_T_DAS, INDEX_T_MAP, SD, EXREC, MINIPH5, Das, SIZE, ARRAY_T, DN, LAT, LON, F
        
        MINIPH5 = None
        ARRAY_T = {}
        
        def print_container (container) :
            keys = container.keys ()
            for k in keys :
                print k, container[k]
                
            print '-' * 80    
        
        get_args ()
        
        initializeExperiment ()
        logging.info ("segd2ph5 {0}".format (PROG_VERSION))
        logging.info ("{0}".format (sys.argv))
        if len (FILES) > 0 :
            RESP = resp (EX.ph5_g_responses)
            rows, keys = EX.ph5_g_receivers.read_index ()
            INDEX_T_DAS = rows_keys (rows, keys)
            rows, keys = EX.ph5_g_maps.read_index ()
            INDEX_T_MAP = rows_keys (rows, keys)     
        
        for f in FILES :
            F = f
            try :
                SIZE = os.path.getsize (f)
            except Exception as e :
                sys.stderr.write ("Error: failed to read {0}, {1}. Skipping...\n".format (f, str (e.message)))
                logging.error ("Error: failed to read {0}, {1}. Skipping...\n".format (f, str (e.message)))
                continue
            
            SD = SegdReader.Reader (infile=f)
            LAT = None; LON = None
            DN = False
            if not SD.isSEGD () :
                sys.stdout.write (":<Error>: {0}\n".format (SD.name ())); sys.stdout.flush ()
                logging.info ("{0} is not a Fairfield SEG-D file. Skipping.".format (SD.name ()))
                continue
        
            try :
                SD.process_general_headers ()
                SD.process_channel_set_descriptors ()
                SD.process_extended_headers ()
                SD.process_external_headers ()
            except SegdReader.InputsError as e :
                sys.stdout.write (":<Error>: {0}\n".format ("".join (e))); sys.stdout.flush ()
                logging.info ("Error: Possible bad SEG-D file -- {0}".format ("".join (e)))
                continue
            
            #Das = (SD.reel_headers.extended_header_3.line_number * 1000) + SD.reel_headers.extended_header_3.receiver_point
            Das = int ("{0:03d}{1:03d}".format (SD.reel_headers.extended_header_3.line_number, 
                                                SD.reel_headers.extended_header_3.receiver_point))
            EXREC = get_current_data_only (SIZE, Das)
            #sys.stderr.write ("Processing: {0}... Size: {1}\n".format (SD.name (), SIZE))
            sys.stdout.write (":<Processing>: {0}\n".format (SD.name ())); sys.stdout.flush ()
            logging.info ("Processing: {0}... Size: {1}\n".format (SD.name (), SIZE))
            if EXREC.filename != MINIPH5 :
                #sys.stderr.write ("Opened: {0}...\n".format (EXREC.filename))
                logging.info ("Opened: {0}...\n".format (EXREC.filename))
                MINIPH5 = EXREC.filename  
                
            n = 0
            trace_headers_list = []
            lat = None; lon = None
            while True :
                #
                if SD.isEOF () :
                    if n != 0 :
                        process_traces (SD.reel_headers, trace_headers_list[0], trace)
                        if DAS_INFO : writeINDEX ()
                    break
                
                try :
                    SD.process_trace_headers ()
                except SegdReader.InputsError as e :
                    #sys.stderr.write ("Error 2: Possible bad SEG-D file -- {0}".format ("".join (e)))
                    sys.stdout.write (":<Error:> {0}\n".format (F)); sys.stdout.flush ()
                    logging.info ("Error: Possible bad SEG-D file -- {0}".format ("".join (e)))
                    break
                    
                if not LAT and not LON :
                    if UTM :
                        #   UTM
                        LAT, LON = utmcsptolatlon (SD.trace_headers.trace_header_N[4].receiver_point_Y_final / 10.,
                                                   SD.trace_headers.trace_header_N[4].receiver_point_X_final / 10.)
                    elif TSPF :
                        #   Texas State Plane coordinates
                        LAT, LON = txncsptolatlon (SD.trace_headers.trace_header_N[4].receiver_point_Y_final / 10.,
                                                   SD.trace_headers.trace_header_N[4].receiver_point_X_final / 10.)
                    else :
                        LAT = SD.trace_headers.trace_header_N[4].receiver_point_Y_final / 10.
                        LON = SD.trace_headers.trace_header_N[4].receiver_point_X_final / 10.
                
                try :    
                    tmp_trace = SD.read_trace (SD.samples)
                except SegdReader.InputsError as e :
                    #sys.stderr.write ("Error 3: Possible bad SEG-D file -- {0}".format ("".join (e)))
                    sys.stdout.write (":<Error:> {0}\n".format (F)); sys.stdout.flush ()
                    logging.info ("Error: Possible bad SEG-D file -- {0}".format ("".join (e)))
                    if n > 0 :
                        process_traces (SD.reel_headers, trace_headers_list[0], trace)
                        if DAS_INFO : writeINDEX ()  
                        
                    break 
                
                trace_headers_list.append (SD.trace_headers)
                if n == 0 :
                    trace = tmp_trace
                    #   Sweetwater kludge
                    #Das = (SD.trace_headers.trace_header_N[0].receiver_line * 1000) + SD.trace_headers.trace_header_N[0].receiver_point
                    Das = int ("{0:03d}{1:03d}".format (SD.reel_headers.extended_header_3.line_number, 
                                                        SD.reel_headers.extended_header_3.receiver_point))                    
                else :
                    trace = npappend (trace, tmp_trace)
                    
                if n >= APPEND or EVERY == True :
                    #print '.',
                    process_traces (SD.reel_headers, trace_headers_list[0], trace)
                    if DAS_INFO : writeINDEX ()
                    n = 0
                    trace_headers_list = []
                    continue
                
                n += 1
                
            update_external_references ()
            
        write_arrays (ARRAY_T)
        seconds = time.time () - then
    
        try :
            EX.ph5close (); EXREC.ph5close ()
        except Exception as e :
            sys.stderr.write ("Warning: {0}\n".format ("".join (e)))
        
        print "Done...{0:b}".format (int (seconds/6.))   #   Minutes X 10
        logging.info ("Done...{0:b}".format (int (seconds/6.)))
        logging.shutdown
            
     
    ##   Profile
    #import cProfile, pstats
    #sys.stderr.write ("Warning: Profiling enabled!\n")
    #cProfile.run ('prof ()', "segd2ph5.profile")
    
    #p = pstats.Stats ("segd2ph5.profile")
    #p.sort_stats('time').print_stats(40)
    ##   Profile stop
    
    #   No profile
    prof ()    
    
    